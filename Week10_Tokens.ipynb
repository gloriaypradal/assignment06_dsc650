{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "julian-founder",
   "metadata": {},
   "source": [
    "# Week 10\n",
    "\n",
    "\n",
    "## Gloria Moore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compact-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from nltk) (2021.4.4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acute-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Airplane Crashes has been decreasing from 1995 to 2020 in about 31% worldwide, this is an indicative that flying is becoming safer over the years\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "powerful-independence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hairy-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    #removing punctuation\n",
    "    res = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    #removing numbers from a string\n",
    "    res = re.sub(r\"\\d\", \"\", res)\n",
    "    # tokenize the sentence\n",
    "    tokens = nltk.word_tokenize(res)\n",
    "       \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "automotive-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "optional-passage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Airplane',\n",
       " 'Crashes',\n",
       " 'has',\n",
       " 'been',\n",
       " 'decreasing',\n",
       " 'from',\n",
       " 'to',\n",
       " 'in',\n",
       " 'about',\n",
       " 'worldwide',\n",
       " 'this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'indicative',\n",
       " 'that',\n",
       " 'flying',\n",
       " 'is',\n",
       " 'becoming',\n",
       " 'safer',\n",
       " 'over',\n",
       " 'the',\n",
       " 'years']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-agriculture",
   "metadata": {},
   "source": [
    "#### Assignment 10.1.b. Implement an `ngram` function that splits tokens into N-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "infectious-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(tokens, n):\n",
    "    ngrams = []\n",
    "    # Create ngrams\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngrams.append(tokens[i:i+n])\n",
    "        \n",
    "    \n",
    "    return [\" \".join(a) for a in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "talented-assets",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Airplane Crashes',\n",
       " 'Crashes has',\n",
       " 'has been',\n",
       " 'been decreasing',\n",
       " 'decreasing from',\n",
       " 'from to',\n",
       " 'to in',\n",
       " 'in about',\n",
       " 'about worldwide',\n",
       " 'worldwide this',\n",
       " 'this is',\n",
       " 'is an',\n",
       " 'an indicative',\n",
       " 'indicative that',\n",
       " 'that flying',\n",
       " 'flying is',\n",
       " 'is becoming',\n",
       " 'becoming safer',\n",
       " 'safer over',\n",
       " 'over the',\n",
       " 'the years']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram(tokens, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-swimming",
   "metadata": {},
   "source": [
    "### Assignment 10.1.c. Implement an one_hot_encode function to create a vector from a numerical vector from a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stainless-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cheeking other book\n",
    "def vectorize(tokens):\n",
    "    return {token: True for token in tokens}\n",
    "vectors = map(vectorize, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "special-genesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from our tex Book p.182\n",
    "\n",
    "def one_hot_encode(tokens, num_words):\n",
    "    token_index = {}\n",
    "    for sample in samples:\n",
    "        for word in sample.split():\n",
    "            if word not in token_index:\n",
    "                token_index[word] = len(token_index) + 1\n",
    "                results = np.zeros(shape = (len(samples), num_words, max(token_index.values()) + 1))\n",
    "    for i, sample in enumerate(samples):\n",
    "        for j, word in list(enumerate(sample.split()))[ :num_words]:\n",
    "            index = token_index.get(word)\n",
    "            results[i, j, index] = 1\n",
    "    return results\n",
    "\n",
    "\n",
    "num_words = 10\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "one_hot_encode(tokens, num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-pledge",
   "metadata": {},
   "source": [
    "##### 10.2. \n",
    "\n",
    "Using listings 6.16, 6.17, and 6.18 in Deep Learning with Python as a guide, train a sequential model with embeddings on the IMDB data found in data/external/imdb/. Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "funded-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p 193\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "accessory-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path(os.getcwd()).absolute()\n",
    "imdb_dir = Path('/home/jovyan/dsc650/data/external/imdb/aclImdb/')\n",
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "amended-plane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "introductory-identification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 0.6971 - acc: 0.5052 - val_loss: 0.6918 - val_acc: 0.5228\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 1s 111ms/step - loss: 0.5246 - acc: 1.0000 - val_loss: 0.6928 - val_acc: 0.5270\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.3124 - acc: 0.9829 - val_loss: 0.7007 - val_acc: 0.5223\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 1s 111ms/step - loss: 0.1420 - acc: 1.0000 - val_loss: 0.6964 - val_acc: 0.5273\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 0.0659 - acc: 1.0000 - val_loss: 0.7061 - val_acc: 0.5322\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 1s 115ms/step - loss: 0.0346 - acc: 1.0000 - val_loss: 0.7069 - val_acc: 0.5323\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.7123 - val_acc: 0.5330\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 1s 118ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.7156 - val_acc: 0.5312\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 1s 110ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.7214 - val_acc: 0.5320\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.7238 - val_acc: 0.5334\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "epochs=10,\n",
    "batch_size=32,\n",
    "validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "different-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.17. p. 194\n",
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "legendary-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.7910 - val_acc: 0.5382\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 1.2721e-04 - acc: 1.0000 - val_loss: 0.7886 - val_acc: 0.5370\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 5.8448e-05 - acc: 1.0000 - val_loss: 0.7979 - val_acc: 0.5359\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 3.5699e-05 - acc: 1.0000 - val_loss: 0.8051 - val_acc: 0.5356\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 2.2800e-05 - acc: 1.0000 - val_loss: 0.8139 - val_acc: 0.5351\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 1.5773e-05 - acc: 1.0000 - val_loss: 0.8209 - val_acc: 0.5365\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 1.0921e-05 - acc: 1.0000 - val_loss: 0.8279 - val_acc: 0.5357\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 7.4389e-06 - acc: 1.0000 - val_loss: 0.8347 - val_acc: 0.5363\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 5.2887e-06 - acc: 1.0000 - val_loss: 0.8423 - val_acc: 0.5363\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 3.9717e-06 - acc: 1.0000 - val_loss: 0.8501 - val_acc: 0.5357\n"
     ]
    }
   ],
   "source": [
    "#6.14 \n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,epochs=10,\n",
    "batch_size=32,\n",
    "validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "comfortable-flashing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.8453 - acc: 0.5405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8452569246292114, 0.5404800176620483]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6.18 p. 195\n",
    "\n",
    "\n",
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-november",
   "metadata": {},
   "source": [
    "#### 10.3\n",
    "\n",
    "Using listing 6.27 in Deep Learning with Python as a guide, fit the same data with an LSTM layer. Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "representative-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.27. p205\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "parallel-shopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "input_train shape: (25000, 500)\n",
      "input_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(\n",
    "num_words=max_features)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fourth-hollow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 69s 425ms/step - loss: 0.5822 - acc: 0.6904 - val_loss: 0.3993 - val_acc: 0.8270\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 62s 396ms/step - loss: 0.2876 - acc: 0.8888 - val_loss: 0.2980 - val_acc: 0.8886\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 63s 399ms/step - loss: 0.2333 - acc: 0.9144 - val_loss: 0.3478 - val_acc: 0.8630\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 62s 393ms/step - loss: 0.1974 - acc: 0.9300 - val_loss: 0.3600 - val_acc: 0.8746\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 63s 401ms/step - loss: 0.1740 - acc: 0.9383 - val_loss: 0.3181 - val_acc: 0.8794\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 62s 393ms/step - loss: 0.1530 - acc: 0.9497 - val_loss: 0.9120 - val_acc: 0.7838\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 62s 396ms/step - loss: 0.1514 - acc: 0.9507 - val_loss: 0.4821 - val_acc: 0.8134\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 61s 392ms/step - loss: 0.1217 - acc: 0.9577 - val_loss: 0.4171 - val_acc: 0.8678\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 62s 394ms/step - loss: 0.1140 - acc: 0.9628 - val_loss: 0.3779 - val_acc: 0.8614\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 62s 395ms/step - loss: 0.0968 - acc: 0.9671 - val_loss: 0.3898 - val_acc: 0.8792\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "loss='binary_crossentropy',\n",
    "metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,epochs=10,batch_size=128,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-healthcare",
   "metadata": {},
   "source": [
    "#### 10.4\n",
    "\n",
    "\n",
    "Using listing 6.46 in Deep Learning with Python as a guide, fit the same data with a simple 1D convnet. Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fatty-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 500)\n",
      "x_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "regulated-pepper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "157/157 [==============================] - 11s 67ms/step - loss: 4.3037 - acc: 0.5013 - val_loss: 0.6917 - val_acc: 0.5374\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 10s 64ms/step - loss: 0.6857 - acc: 0.5727 - val_loss: 0.6747 - val_acc: 0.6302\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 10s 65ms/step - loss: 0.6527 - acc: 0.7092 - val_loss: 0.6401 - val_acc: 0.6732\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 10s 64ms/step - loss: 0.5941 - acc: 0.7867 - val_loss: 0.5362 - val_acc: 0.7934\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 10s 64ms/step - loss: 0.4711 - acc: 0.8360 - val_loss: 0.4328 - val_acc: 0.8338\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 10s 61ms/step - loss: 0.3647 - acc: 0.8686 - val_loss: 0.4064 - val_acc: 0.8498\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.3253 - acc: 0.8869 - val_loss: 0.4117 - val_acc: 0.8586\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 10s 62ms/step - loss: 0.2836 - acc: 0.9052 - val_loss: 0.4137 - val_acc: 0.8610\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2568 - acc: 0.9127 - val_loss: 0.4364 - val_acc: 0.8644\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 10s 63ms/step - loss: 0.2428 - acc: 0.9211 - val_loss: 0.4462 - val_acc: 0.8718\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "epochs=10,\n",
    "batch_size=128,\n",
    "validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-offset",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
